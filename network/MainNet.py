# network/MainNet.py - æ”¹è¿›çš„ä¸‰åˆ›æ–°ç‚¹é›†æˆç‰ˆæœ¬
# è§£å†³æ¨¡å—ç»„åˆè´Ÿæ•ˆåº”é—®é¢˜ï¼Œå®ç°çœŸæ­£çš„ååŒå·¥ä½œ

import torch
import torch.nn as nn
import torch.nn.functional as F
from efficientnet_pytorch import EfficientNet
from .enhanced_modules_memory import (
    DynamicSpatioTemporalMemory, 
    MultiQueryFeatureFusion, 
    AdaptiveChannelGroupingMechanism,
    SimplifiedAdaptiveChannelGroupingMechanism
)

class MemoryForensicsNet(nn.Module):
    """
    æ”¹è¿›çš„MemoryForensicsä¸»ç½‘ç»œ - è§£å†³æ¨¡å—ååŒé—®é¢˜
    
    ä¸»è¦æ”¹è¿›ï¼š
    1. å¹¶è¡Œæ¨¡å—è®¾è®¡ - æ‰€æœ‰æ¨¡å—å¹¶è¡Œå¤„ç†ï¼Œé¿å…ç´¯ç§¯è¯¯å·®
    2. æ¨¡å—é—´é€šä¿¡æœºåˆ¶ - å¼•å…¥äº¤å‰æ³¨æ„åŠ›å®ç°æ¨¡å—åä½œ
    3. ä¸“é—¨åŒ–çº¦æŸ - å¼ºåˆ¶æ¨¡å—å­¦ä¹ ä¸åŒç‰¹å¾ï¼Œé¿å…å†—ä½™
    4. åŠ¨æ€æƒé‡èåˆ - æ ¹æ®æ¨¡å—ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´æƒé‡
    5. æ¸è¿›å¼è®­ç»ƒæ”¯æŒ - æ”¯æŒåˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥
    """
    
    def __init__(self, num_classes=2, drop_rate=0.3, 
                 enable_dstm=True, enable_mqff=True, enable_acgm=True,
                 dstm_config=None, mqff_config=None, acgm_config=None):
        super(MemoryForensicsNet, self).__init__()
        
        self.num_classes = num_classes
        self.enable_dstm = enable_dstm
        self.enable_mqff = enable_mqff
        self.enable_acgm = enable_acgm
        
        # é»˜è®¤é…ç½®
        self.dstm_config = dstm_config or {
            'memory_size': 128,  # å‡å°‘è®°å¿†å¤§å°æé«˜ä¸“é—¨åŒ–
            'memory_dim': 256,   # å‡å°‘ç»´åº¦é¿å…è¿‡æ‹Ÿåˆ
            'update_rate': 0.1,
            'temperature': 1.0,
            'update_threshold': 0.8
        }
        
        self.mqff_config = mqff_config or {
            'num_queries': 6,    # å‡å°‘æŸ¥è¯¢æ•°é‡æé«˜ä¸“é—¨åŒ–
            'query_dim': 256,
            'num_heads': 8,
            'dropout': 0.1
        }
        
        self.acgm_config = acgm_config or {
            'num_groups': 8,
            'reduction_ratio': 16,
            'routing_iterations': 3,
            'use_simplified': False
        }
        
        # EfficientNet-B4 éª¨å¹²ç½‘ç»œ
        self.backbone = EfficientNet.from_pretrained('efficientnet-b4')
        self.feature_dim = self.backbone._fc.in_features
        self.backbone._fc = nn.Identity()
        
        # è·å–å·ç§¯ç‰¹å¾å›¾çš„ç»´åº¦
        with torch.no_grad():
            test_input = torch.randn(1, 3, 224, 224)
            conv_features = self.backbone.extract_features(test_input)
            self.conv_features_dim = conv_features.shape[1]
        
        print(f"EfficientNet-B4 feature dimensions: {self.conv_features_dim}")
        
        # åˆ›æ–°æ¨¡å—åˆå§‹åŒ–
        self._initialize_innovation_modules()
        
        # æ¨¡å—é—´é€šä¿¡ä¸­å¿ƒ
        self.communication_hub = ModuleCommunicationHub(
            self.conv_features_dim,
            enabled_modules=[enable_dstm, enable_mqff, enable_acgm]
        )
        
        # ä¸“é—¨åŒ–çº¦æŸæ¨¡å—
        self.specialization_constraint = SpecializationConstraint(
            self.conv_features_dim
        )
        
        # åŠ¨æ€æƒé‡èåˆå™¨
        self.dynamic_fusion = DynamicWeightFusion(
            self.conv_features_dim,
            num_modules=sum([enable_dstm, enable_mqff, enable_acgm])
        )
        
        # æ”¹è¿›çš„æ™ºèƒ½åˆ†ç±»å™¨
        self.intelligent_classifier = ImprovedIntelligentClassifier(
            input_channels=self.conv_features_dim,
            num_classes=num_classes,
            drop_rate=drop_rate,
            num_modules=sum([enable_dstm, enable_mqff, enable_acgm])
        )
        
        # è¾…åŠ©åˆ†ç±»å™¨ï¼ˆç”¨äºå¤šä»»åŠ¡å­¦ä¹ ï¼‰
        self.auxiliary_classifiers = nn.ModuleDict()
        if enable_dstm:
            self.auxiliary_classifiers['dstm'] = self._create_auxiliary_classifier(drop_rate)
        if enable_mqff:
            self.auxiliary_classifiers['mqff'] = self._create_auxiliary_classifier(drop_rate)
        if enable_acgm:
            self.auxiliary_classifiers['acgm'] = self._create_auxiliary_classifier(drop_rate)
        
        # ç‰¹å¾åˆ†æå™¨
        self.feature_analyzer = FeatureAnalyzer(self.conv_features_dim)
        
        print(f"Improved MemoryForensics Network Initialized:")
        print(f"  - Innovation Modules: DSTM={enable_dstm}, MQFF={enable_mqff}, ACGM={enable_acgm}")
        print(f"  - Auxiliary Classifiers: {len(self.auxiliary_classifiers)}")
        print(f"  - Total Parameters: {sum(p.numel() for p in self.parameters()):,}")
    
    def _initialize_innovation_modules(self):
        """åˆå§‹åŒ–åˆ›æ–°æ¨¡å—"""
        
        # 1. ä¸“é—¨åŒ–çš„DSTM - ä¸“æ³¨äºæ—¶åºè®°å¿†
        if self.enable_dstm:
            self.dstm = DynamicSpatioTemporalMemory(
                input_channels=self.conv_features_dim,
                **self.dstm_config
            )
            print(f"âœ… DSTM initialized (ä¸“æ³¨æ—¶åºè®°å¿†)")
        
        # 2. ä¸“é—¨åŒ–çš„MQFF - ä¸“æ³¨äºå¤šå°ºåº¦æŸ¥è¯¢
        if self.enable_mqff:
            self.mqff = MultiQueryFeatureFusion(
                input_channels=self.conv_features_dim,
                **self.mqff_config
            )
            print(f"âœ… MQFF initialized (ä¸“æ³¨å¤šå°ºåº¦æŸ¥è¯¢)")
        
        # 3. ä¸“é—¨åŒ–çš„ACGM - ä¸“æ³¨äºé€šé“è‡ªé€‚åº”
        if self.enable_acgm:
            if self.acgm_config.get('use_simplified', False):
                self.acgm = SimplifiedAdaptiveChannelGroupingMechanism(
                    input_channels=self.conv_features_dim,
                    num_groups=self.acgm_config['num_groups'],
                    reduction_ratio=self.acgm_config['reduction_ratio']
                )
                print(f"âœ… Simplified ACGM initialized (ä¸“æ³¨é€šé“è‡ªé€‚åº”)")
            else:
                try:
                    self.acgm = AdaptiveChannelGroupingMechanism(
                        input_channels=self.conv_features_dim,
                        num_groups=self.acgm_config['num_groups'],
                        reduction_ratio=self.acgm_config['reduction_ratio'],
                        routing_iterations=self.acgm_config['routing_iterations']
                    )
                    print(f"âœ… Full ACGM initialized (ä¸“æ³¨é€šé“è‡ªé€‚åº”)")
                except Exception as e:
                    print(f"âš ï¸ Full ACGM failed, using simplified version: {e}")
                    self.acgm = SimplifiedAdaptiveChannelGroupingMechanism(
                        input_channels=self.conv_features_dim,
                        num_groups=self.acgm_config['num_groups'],
                        reduction_ratio=self.acgm_config['reduction_ratio']
                    )
    
    def _create_auxiliary_classifier(self, drop_rate):
        """åˆ›å»ºè¾…åŠ©åˆ†ç±»å™¨"""
        return nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Dropout(drop_rate),
            nn.Linear(self.conv_features_dim, self.conv_features_dim // 2),
            nn.ReLU(),
            nn.Dropout(drop_rate * 0.5),
            nn.Linear(self.conv_features_dim // 2, self.num_classes)
        )
    
    def forward(self, x, training_stage='full', return_aux_outputs=False, return_analysis=False):
        """
        æ”¹è¿›çš„å‰å‘ä¼ æ’­ - å…¼å®¹åŸæœ‰æ¥å£
        
        Args:
            x: è¾“å…¥å›¾åƒ (B, 3, 224, 224)
            training_stage: è®­ç»ƒé˜¶æ®µ ('single', 'progressive', 'full')
            return_aux_outputs: æ˜¯å¦è¿”å›è¾…åŠ©è¾“å‡º
            return_analysis: æ˜¯å¦è¿”å›ç‰¹å¾åˆ†æ
            
        Returns:
            æ ¹æ®å‚æ•°è¿”å›ä¸åŒçš„ç»“æœç»„åˆ
        """
        # 1. éª¨å¹²ç½‘ç»œç‰¹å¾æå–
        backbone_features = self.backbone.extract_features(x)  # (B, C, H, W)
        
        # 2. è®°å½•åŸå§‹ç‰¹å¾
        original_features = backbone_features.clone() if return_analysis else None
        
        # 3. å¹¶è¡Œæ¨¡å—å¤„ç† - é¿å…ç´¯ç§¯è¯¯å·®
        module_features = {}
        aux_outputs = {}
        
        # 3.1 å¹¶è¡Œå¤„ç†å„æ¨¡å—
        if self.enable_dstm:
            dstm_features = self.dstm(backbone_features)  # ç›´æ¥ä»éª¨å¹²ç‰¹å¾å¤„ç†
            module_features['dstm'] = dstm_features
            if return_aux_outputs:
                aux_outputs['dstm'] = self.auxiliary_classifiers['dstm'](dstm_features)
        
        if self.enable_mqff:
            mqff_features = self.mqff(backbone_features)  # ç›´æ¥ä»éª¨å¹²ç‰¹å¾å¤„ç†
            module_features['mqff'] = mqff_features
            if return_aux_outputs:
                aux_outputs['mqff'] = self.auxiliary_classifiers['mqff'](mqff_features)
        
        if self.enable_acgm:
            acgm_features = self.acgm(backbone_features)  # ç›´æ¥ä»éª¨å¹²ç‰¹å¾å¤„ç†
            module_features['acgm'] = acgm_features
            if return_aux_outputs:
                aux_outputs['acgm'] = self.auxiliary_classifiers['acgm'](acgm_features)
        
        # 4. æ¨¡å—é—´é€šä¿¡ - å®ç°åä½œ
        if len(module_features) > 1:
            communicated_features = self.communication_hub(module_features)
        else:
            communicated_features = module_features
        
        # 5. è®¡ç®—ä¸“é—¨åŒ–çº¦æŸæŸå¤±
        specialization_loss = self.specialization_constraint(module_features)
        
        # 6. åŠ¨æ€æƒé‡èåˆ
        fused_features = self.dynamic_fusion(
            backbone_features, communicated_features, training_stage
        )
        
        # 7. æ™ºèƒ½åˆ†ç±»
        main_output = self.intelligent_classifier(fused_features, communicated_features)
        
        # 8. ç‰¹å¾åˆ†æ
        analysis = None
        if return_analysis:
            analysis = self.feature_analyzer(
                original_features, fused_features, communicated_features
            )
        
        # 9. è¿”å›ç»“æœ - å…¼å®¹åŸæœ‰æ¥å£
        if return_aux_outputs and return_analysis:
            if self.training:
                return main_output, aux_outputs, analysis, specialization_loss
            else:
                return main_output, aux_outputs, analysis
        elif return_aux_outputs:
            if self.training:
                return main_output, aux_outputs, specialization_loss
            else:
                return main_output, aux_outputs
        elif return_analysis:
            if self.training:
                return main_output, analysis, specialization_loss
            else:
                return main_output, analysis
        else:
            # æœ€å¸¸è§çš„æƒ…å†µ - åªè¿”å›ä¸»è¾“å‡ºï¼ˆæµ‹è¯•æ—¶ä½¿ç”¨ï¼‰
            return main_output
    
    # ä¿æŒåŸæœ‰çš„å…¶ä»–æ–¹æ³•
    def extract_features(self, x):
        """æå–å¤šå±‚ç‰¹å¾ç”¨äºåˆ†æå’Œå¯è§†åŒ–"""
        with torch.no_grad():
            backbone_features = self.backbone.extract_features(x)
            
            features = {
                'backbone': backbone_features,
                'modules': {}
            }
            
            # å¹¶è¡Œæå–å„æ¨¡å—ç‰¹å¾
            if self.enable_dstm:
                features['modules']['dstm'] = self.dstm(backbone_features)
            if self.enable_mqff:
                features['modules']['mqff'] = self.mqff(backbone_features)
            if self.enable_acgm:
                features['modules']['acgm'] = self.acgm(backbone_features)
            
            # é€šä¿¡åç‰¹å¾
            if len(features['modules']) > 1:
                features['communicated'] = self.communication_hub(features['modules'])
            else:
                features['communicated'] = features['modules']
            
            # èåˆç‰¹å¾
            features['fused'] = self.dynamic_fusion(
                backbone_features, features['communicated'], 'full'
            )
            
            return features
    
    def get_memory_status(self):
        """è·å–è®°å¿†åº“çŠ¶æ€"""
        if self.enable_dstm and hasattr(self.dstm, 'memory_bank'):
            with torch.no_grad():
                return {
                    'memory_bank': self.dstm.memory_bank.data.clone(),
                    'memory_age': self.dstm.memory_age.data.clone(),
                    'memory_quality': self.dstm.memory_quality.data.clone(),
                    'memory_activation': self.dstm.memory_activation.data.clone()
                }
        return None
    
    def reset_memory(self):
        """é‡ç½®è®°å¿†åº“"""
        if self.enable_dstm and hasattr(self.dstm, '_initialize_memory'):
            self.dstm._initialize_memory()
            print("âœ… Memory bank reset successfully")


class ModuleCommunicationHub(nn.Module):
    """æ¨¡å—é—´é€šä¿¡ä¸­å¿ƒ - å®ç°æ¨¡å—åä½œ"""
    
    def __init__(self, feature_dim, enabled_modules):
        super(ModuleCommunicationHub, self).__init__()
        
        self.feature_dim = feature_dim
        self.enabled_modules = enabled_modules
        self.num_modules = sum(enabled_modules)
        
        if self.num_modules <= 1:
            return
        
        # äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ - å®ç°æ¨¡å—é—´ä¿¡æ¯äº¤æ¢
        self.cross_attention = nn.ModuleDict()
        module_names = []
        if enabled_modules[0]:  # DSTM
            module_names.append('dstm')
        if enabled_modules[1]:  # MQFF
            module_names.append('mqff')
        if enabled_modules[2]:  # ACGM
            module_names.append('acgm')
        
        # ä¸ºæ¯å¯¹æ¨¡å—åˆ›å»ºäº¤å‰æ³¨æ„åŠ›
        for i, name_i in enumerate(module_names):
            for j, name_j in enumerate(module_names):
                if i != j:
                    self.cross_attention[f'{name_i}_to_{name_j}'] = nn.MultiheadAttention(
                        embed_dim=feature_dim,
                        num_heads=8,
                        batch_first=True,
                        dropout=0.1
                    )
        
        # é€šä¿¡æƒé‡çŸ©é˜µ
        self.communication_weights = nn.Parameter(
            torch.eye(self.num_modules) + 0.1 * torch.randn(self.num_modules, self.num_modules)
        )
        
        # ä¿¡æ¯é—¨æ§
        self.info_gate = nn.Sequential(
            nn.Linear(feature_dim, feature_dim // 4),
            nn.ReLU(),
            nn.Linear(feature_dim // 4, 1),
            nn.Sigmoid()
        )
    
    def forward(self, module_features):
        """æ¨¡å—é—´é€šä¿¡"""
        if len(module_features) <= 1:
            return module_features
        
        B, C, H, W = next(iter(module_features.values())).shape
        
        # è½¬æ¢ä¸ºåºåˆ—æ ¼å¼ç”¨äºæ³¨æ„åŠ›è®¡ç®—
        feature_tokens = {}
        for name, features in module_features.items():
            # ä½¿ç”¨è‡ªé€‚åº”æ± åŒ–å‡å°‘è®¡ç®—é‡
            pooled = F.adaptive_avg_pool2d(features, (4, 4))  # (B, C, 4, 4)
            tokens = pooled.flatten(2).transpose(1, 2)  # (B, 16, C)
            feature_tokens[name] = tokens
        
        # äº¤å‰æ³¨æ„åŠ›é€šä¿¡
        communicated_features = {}
        module_names = list(module_features.keys())
        
        for i, name_i in enumerate(module_names):
            enhanced_token = feature_tokens[name_i]
            
            # ä¸å…¶ä»–æ¨¡å—è¿›è¡Œäº¤å‰æ³¨æ„åŠ›
            for j, name_j in enumerate(module_names):
                if i != j:
                    key = f'{name_i}_to_{name_j}'
                    if key in self.cross_attention:
                        attended, _ = self.cross_attention[key](
                            enhanced_token, feature_tokens[name_j], feature_tokens[name_j]
                        )
                        # åŠ æƒèåˆ
                        enhanced_token = enhanced_token + 0.1 * attended
            
            # è½¬æ¢å›ç©ºé—´æ ¼å¼
            enhanced_spatial = enhanced_token.transpose(1, 2).view(B, C, 4, 4)
            enhanced_spatial = F.interpolate(enhanced_spatial, size=(H, W), mode='bilinear', align_corners=False)
            
            # ä¿¡æ¯é—¨æ§
            gate = self.info_gate(F.adaptive_avg_pool2d(enhanced_spatial, 1).flatten(1))
            gate = gate.unsqueeze(-1).unsqueeze(-1)
            
            # é—¨æ§èåˆ
            communicated_features[name_i] = gate * enhanced_spatial + (1 - gate) * module_features[name_i]
        
        return communicated_features


class SpecializationConstraint(nn.Module):
    """ä¸“é—¨åŒ–çº¦æŸæ¨¡å— - å¼ºåˆ¶æ¨¡å—å­¦ä¹ ä¸åŒç‰¹å¾"""
    
    def __init__(self, feature_dim):
        super(SpecializationConstraint, self).__init__()
        self.feature_dim = feature_dim
        
    def forward(self, module_features):
        """è®¡ç®—ä¸“é—¨åŒ–çº¦æŸæŸå¤±"""
        if len(module_features) <= 1:
            return torch.tensor(0.0, device=next(iter(module_features.values())).device)
        
        # è®¡ç®—ç‰¹å¾è¡¨ç¤ºçš„æ­£äº¤æ€§
        feature_vectors = []
        for name, features in module_features.items():
            # è½¬æ¢ä¸ºå…¨å±€ç‰¹å¾å‘é‡
            global_vector = F.adaptive_avg_pool2d(features, 1).flatten(1)
            global_vector = F.normalize(global_vector, dim=1)
            feature_vectors.append(global_vector)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_loss = 0.0
        count = 0
        
        for i in range(len(feature_vectors)):
            for j in range(i+1, len(feature_vectors)):
                # è®¡ç®—æ‰¹æ¬¡å†…çš„å¹³å‡ç›¸ä¼¼åº¦
                similarity = F.cosine_similarity(feature_vectors[i], feature_vectors[j], dim=1)
                similarity_loss += similarity.abs().mean()
                count += 1
        
        return similarity_loss / count if count > 0 else torch.tensor(0.0, device=feature_vectors[0].device)


class DynamicWeightFusion(nn.Module):
    """åŠ¨æ€æƒé‡èåˆå™¨ - è‡ªé€‚åº”è°ƒæ•´æ¨¡å—é‡è¦æ€§"""
    
    def __init__(self, feature_dim, num_modules):
        super(DynamicWeightFusion, self).__init__()
        
        self.feature_dim = feature_dim
        self.num_modules = num_modules
        
        if num_modules == 0:
            return
        
        # è¾“å…¥éš¾åº¦è¯„ä¼°å™¨
        self.difficulty_estimator = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(feature_dim, feature_dim // 4),
            nn.ReLU(),
            nn.Linear(feature_dim // 4, 1),
            nn.Sigmoid()
        )
        
        # æ¨¡å—ç½®ä¿¡åº¦è¯„ä¼°å™¨
        self.confidence_estimator = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(feature_dim, feature_dim // 4),
            nn.ReLU(),
            nn.Linear(feature_dim // 4, 1),
            nn.Sigmoid()
        )
        
        # åŠ¨æ€æƒé‡ç”Ÿæˆå™¨
        self.weight_generator = nn.Sequential(
            nn.Linear(num_modules + 1, num_modules * 2),
            nn.ReLU(),
            nn.Linear(num_modules * 2, num_modules),
            nn.Softmax(dim=1)
        )
        
        # ç‰¹å¾èåˆç½‘ç»œ
        self.fusion_conv = nn.Sequential(
            nn.Conv2d(feature_dim, feature_dim, 3, padding=1),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(),
            nn.Conv2d(feature_dim, feature_dim, 1),
            nn.BatchNorm2d(feature_dim)
        )
        
        # æ®‹å·®é—¨æ§
        self.residual_gate = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(feature_dim, 1),
            nn.Sigmoid()
        )
    
    def forward(self, backbone_features, module_features, training_stage='full'):
        """åŠ¨æ€æƒé‡èåˆ"""
        if not module_features or self.num_modules == 0:
            return backbone_features
        
        B, C, H, W = backbone_features.shape
        
        # 1. è¯„ä¼°è¾“å…¥éš¾åº¦
        difficulty = self.difficulty_estimator(backbone_features)
        
        # 2. è¯„ä¼°å„æ¨¡å—ç½®ä¿¡åº¦
        confidences = []
        for name, features in module_features.items():
            conf = self.confidence_estimator(features)
            confidences.append(conf)
        
        confidences = torch.cat(confidences, dim=1)
        
        # 3. ç”ŸæˆåŠ¨æ€æƒé‡
        weight_input = torch.cat([difficulty, confidences], dim=1)
        dynamic_weights = self.weight_generator(weight_input)
        
        # 4. åŠ æƒèåˆæ¨¡å—ç‰¹å¾
        fused_features = backbone_features.clone()
        
        for i, (name, features) in enumerate(module_features.items()):
            weight = dynamic_weights[:, i:i+1].unsqueeze(-1).unsqueeze(-1)
            fused_features = fused_features + weight * features
        
        # 5. ç‰¹å¾èåˆå¤„ç†
        enhanced_features = self.fusion_conv(fused_features)
        
        # 6. æ®‹å·®é—¨æ§
        gate = self.residual_gate(enhanced_features)
        gate = gate.unsqueeze(-1).unsqueeze(-1)
        
        final_features = gate * enhanced_features + (1 - gate) * backbone_features
        
        return final_features


class ImprovedIntelligentClassifier(nn.Module):
    """æ”¹è¿›çš„æ™ºèƒ½åˆ†ç±»å™¨"""
    
    def __init__(self, input_channels, num_classes, drop_rate=0.3, num_modules=3):
        super(ImprovedIntelligentClassifier, self).__init__()
        
        self.input_channels = input_channels
        self.num_classes = num_classes
        self.num_modules = num_modules
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        self.multi_scale_pools = nn.ModuleList([
            nn.AdaptiveAvgPool2d(1),
            nn.AdaptiveAvgPool2d(2),
            nn.AdaptiveAvgPool2d(4)
        ])
        
        # ç‰¹å¾èåˆ
        multi_scale_dim = input_channels * (1 + 4 + 16)
        self.feature_fusion = nn.Sequential(
            nn.Linear(multi_scale_dim, input_channels),
            nn.BatchNorm1d(input_channels),
            nn.ReLU(),
            nn.Dropout(drop_rate)
        )
        
        # æ¨¡å—æ„ŸçŸ¥æ³¨æ„åŠ›
        if num_modules > 1:
            self.module_attention = nn.Sequential(
                nn.Linear(input_channels, input_channels // 2),
                nn.ReLU(),
                nn.Linear(input_channels // 2, num_modules),
                nn.Softmax(dim=1)
            )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(input_channels, input_channels // 2),
            nn.BatchNorm1d(input_channels // 2),
            nn.ReLU(),
            nn.Dropout(drop_rate * 0.5),
            nn.Linear(input_channels // 2, input_channels // 4),
            nn.BatchNorm1d(input_channels // 4),
            nn.ReLU(),
            nn.Dropout(drop_rate * 0.25),
            nn.Linear(input_channels // 4, num_classes)
        )
    
    def forward(self, features, module_features=None):
        """å‰å‘ä¼ æ’­"""
        # å¤šå°ºåº¦ç‰¹å¾æå–
        multi_scale_features = []
        for pool in self.multi_scale_pools:
            pooled = pool(features)
            flattened = pooled.flatten(1)
            multi_scale_features.append(flattened)
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat(multi_scale_features, dim=1)
        fused_features = self.feature_fusion(combined_features)
        
        # æ¨¡å—æ„ŸçŸ¥æ³¨æ„åŠ›
        if hasattr(self, 'module_attention') and module_features and len(module_features) > 1:
            attention_weights = self.module_attention(fused_features)
            # æ³¨æ„åŠ›æƒé‡å¯ä»¥ç”¨äºè¿›ä¸€æ­¥çš„ç‰¹å¾è°ƒåˆ¶
            
        # åˆ†ç±»
        output = self.classifier(fused_features)
        return output


class FeatureAnalyzer(nn.Module):
    """ç‰¹å¾åˆ†æå™¨"""
    
    def __init__(self, feature_dim):
        super(FeatureAnalyzer, self).__init__()
        
        self.feature_dim = feature_dim
        
        # ç‰¹å¾ç»Ÿè®¡åˆ†æ
        self.stats_analyzer = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(feature_dim, feature_dim // 4),
            nn.ReLU(),
            nn.Linear(feature_dim // 4, 4)
        )
        
        # ç‰¹å¾ç›¸ä¼¼æ€§åˆ†æ
        self.similarity_analyzer = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(feature_dim, feature_dim // 2),
            nn.ReLU(),
            nn.Linear(feature_dim // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, original_features, enhanced_features, module_features):
        """åˆ†æç‰¹å¾"""
        analysis = {
            'original_stats': self.stats_analyzer(original_features),
            'enhanced_stats': self.stats_analyzer(enhanced_features),
            'enhancement_similarity': self.similarity_analyzer(enhanced_features - original_features)
        }
        
        for module_name, features in module_features.items():
            analysis[f'{module_name}_stats'] = self.stats_analyzer(features)
        
        return analysis


# å·¥å‚å‡½æ•°å’Œé…ç½®ä¿æŒä¸å˜
def create_memory_forensics_net(num_classes=2, drop_rate=0.3,
                               enable_dstm=True, enable_mqff=True, enable_acgm=True,
                               dstm_config=None, mqff_config=None, acgm_config=None):
    """åˆ›å»ºæ”¹è¿›çš„MemoryForensicsç½‘ç»œ"""
    return MemoryForensicsNet(
        num_classes=num_classes,
        drop_rate=drop_rate,
        enable_dstm=enable_dstm,
        enable_mqff=enable_mqff,
        enable_acgm=enable_acgm,
        dstm_config=dstm_config,
        mqff_config=mqff_config,
        acgm_config=acgm_config
    )


# ä¿æŒåŸæœ‰çš„é¢„å®šä¹‰é…ç½®
PRESET_CONFIGS = {
    'lightweight': {
        'dstm_config': {
            'memory_size': 64,
            'memory_dim': 128,
            'update_rate': 0.1,
            'temperature': 1.0,
            'update_threshold': 0.8
        },
        'mqff_config': {
            'num_queries': 4,
            'query_dim': 128,
            'num_heads': 4,
            'dropout': 0.1
        },
        'acgm_config': {
            'num_groups': 4,
            'reduction_ratio': 16,
            'routing_iterations': 2,
            'use_simplified': True
        }
    },
    'standard': {
        'dstm_config': {
            'memory_size': 128,
            'memory_dim': 256,
            'update_rate': 0.1,
            'temperature': 1.0,
            'update_threshold': 0.8
        },
        'mqff_config': {
            'num_queries': 6,
            'query_dim': 256,
            'num_heads': 8,
            'dropout': 0.1
        },
        'acgm_config': {
            'num_groups': 8,
            'reduction_ratio': 16,
            'routing_iterations': 3,
            'use_simplified': False
        }
    },
    'high_performance': {
        'dstm_config': {
            'memory_size': 256,
            'memory_dim': 512,
            'update_rate': 0.05,
            'temperature': 0.5,
            'update_threshold': 0.9
        },
        'mqff_config': {
            'num_queries': 8,
            'query_dim': 512,
            'num_heads': 16,
            'dropout': 0.05
        },
        'acgm_config': {
            'num_groups': 16,
            'reduction_ratio': 8,
            'routing_iterations': 5,
            'use_simplified': False
        }
    }
}


def create_preset_model(preset='standard', num_classes=2, drop_rate=0.3,
                       enable_dstm=True, enable_mqff=True, enable_acgm=True):
    """åˆ›å»ºé¢„è®¾é…ç½®çš„æ¨¡å‹"""
    if preset not in PRESET_CONFIGS:
        raise ValueError(f"Unknown preset: {preset}. Available: {list(PRESET_CONFIGS.keys())}")
    
    config = PRESET_CONFIGS[preset]
    
    return create_memory_forensics_net(
        num_classes=num_classes,
        drop_rate=drop_rate,
        enable_dstm=enable_dstm,
        enable_mqff=enable_mqff,
        enable_acgm=enable_acgm,
        **config
    )


# æ”¹è¿›çš„è®­ç»ƒç­–ç•¥ç±»
class ImprovedTrainingStrategy:
    """æ”¹è¿›çš„è®­ç»ƒç­–ç•¥ - è§£å†³æ¨¡å—ååŒé—®é¢˜"""
    
    def __init__(self, model, optimizer, device, training_config=None):
        self.model = model
        self.optimizer = optimizer
        self.device = device
        self.classification_criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒé…ç½®
        self.config = training_config or {
            'specialization_weight': 0.2,
            'auxiliary_weight': 0.3,
            'weight_decay_schedule': True,
            'progressive_training': True
        }
        
        # æŸå¤±æƒé‡è°ƒåº¦
        self.loss_weights = {
            'classification': 1.0,
            'auxiliary': self.config['auxiliary_weight'],
            'specialization': self.config['specialization_weight']
        }
    
    def compute_loss(self, inputs, targets, epoch, total_epochs):
        """è®¡ç®—æ”¹è¿›çš„æŸå¤±å‡½æ•°"""
        
        # æ¸è¿›å¼è®­ç»ƒç­–ç•¥
        if self.config['progressive_training']:
            if epoch < total_epochs * 0.3:
                training_stage = 'specialization'  # å‰30%ä¸“æ³¨ä¸“é—¨åŒ–
            elif epoch < total_epochs * 0.7:
                training_stage = 'communication'   # ä¸­40%å¯ç”¨é€šä¿¡
            else:
                training_stage = 'full'           # å30%å…¨é¢ä¼˜åŒ–
        else:
            training_stage = 'full'
        
        # å‰å‘ä¼ æ’­
        if self.model.training:
            main_output, aux_outputs, specialization_loss = self.model(
                inputs, training_stage=training_stage, return_aux_outputs=True
            )
        else:
            main_output = self.model(inputs, training_stage=training_stage)
            aux_outputs = {}
            specialization_loss = torch.tensor(0.0, device=inputs.device)
        
        # ä¸»åˆ†ç±»æŸå¤±
        main_loss = self.classification_criterion(main_output, targets)
        
        # è¾…åŠ©åˆ†ç±»æŸå¤±
        aux_loss = 0.0
        if aux_outputs:
            for aux_output in aux_outputs.values():
                aux_loss += self.classification_criterion(aux_output, targets)
            aux_loss /= len(aux_outputs)
        
        # åŠ¨æ€æƒé‡è°ƒæ•´
        progress = epoch / total_epochs
        
        # ä¸“é—¨åŒ–æƒé‡éšè®­ç»ƒè¿›ç¨‹è¡°å‡
        if self.config['weight_decay_schedule']:
            dynamic_spec_weight = self.loss_weights['specialization'] * (1 - progress)
        else:
            dynamic_spec_weight = self.loss_weights['specialization']
        
        # è¾…åŠ©æƒé‡åœ¨ä¸­æœŸæœ€å¤§
        if training_stage == 'communication':
            dynamic_aux_weight = self.loss_weights['auxiliary'] * 1.2
        else:
            dynamic_aux_weight = self.loss_weights['auxiliary']
        
        # æ€»æŸå¤±
        total_loss = (
            self.loss_weights['classification'] * main_loss +
            dynamic_aux_weight * aux_loss +
            dynamic_spec_weight * specialization_loss
        )
        
        return total_loss, {
            'main_loss': main_loss.item(),
            'aux_loss': aux_loss.item() if isinstance(aux_loss, torch.Tensor) else aux_loss,
            'spec_loss': specialization_loss.item(),
            'training_stage': training_stage,
            'spec_weight': dynamic_spec_weight,
            'aux_weight': dynamic_aux_weight
        }
    
    def should_update_memory(self, epoch, total_epochs):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°è®°å¿†åº“"""
        # åœ¨è®­ç»ƒå‰æœŸæ›´é¢‘ç¹åœ°æ›´æ–°è®°å¿†åº“
        if epoch < total_epochs * 0.5:
            return True
        else:
            return epoch % 2 == 0  # åæœŸæ¯éš”ä¸€ä¸ªepochæ›´æ–°


# ä¿æŒåŸæœ‰çš„ç‰¹å¾èåˆæ¨¡å—ï¼Œä½†ç®€åŒ–å®ç°
class FeatureFusionModule(nn.Module):
    """ç®€åŒ–çš„ç‰¹å¾èåˆæ¨¡å—"""
    
    def __init__(self, input_channels, enable_dstm=True, enable_mqff=True, enable_acgm=True):
        super(FeatureFusionModule, self).__init__()
        
        self.enable_dstm = enable_dstm
        self.enable_mqff = enable_mqff
        self.enable_acgm = enable_acgm
        
        # ç®€åŒ–çš„èåˆç½‘ç»œ
        self.fusion_conv = nn.Sequential(
            nn.Conv2d(input_channels, input_channels, 3, padding=1),
            nn.BatchNorm2d(input_channels),
            nn.ReLU(),
            nn.Conv2d(input_channels, input_channels, 1),
            nn.BatchNorm2d(input_channels)
        )
        
        # æ®‹å·®é—¨æ§
        self.residual_gate = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(input_channels, 1),
            nn.Sigmoid()
        )
    
    def forward(self, enhanced_features, original_features, module_features=None):
        """ç‰¹å¾èåˆ"""
        fused = self.fusion_conv(enhanced_features)
        gate = self.residual_gate(fused)
        gate = gate.unsqueeze(-1).unsqueeze(-1)
        
        output = gate * fused + (1 - gate) * original_features
        return output


# æµ‹è¯•å‡½æ•°
def test_improved_model():
    """æµ‹è¯•æ”¹è¿›çš„æ¨¡å‹"""
    print("ğŸ§ª Testing Improved MemoryForensics Model...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æµ‹è¯•æ ‡å‡†é…ç½®
    print("\nğŸ”¬ Testing Improved Configuration...")
    try:
        model = create_preset_model('standard', num_classes=2).to(device)
        
        # æµ‹è¯•è¾“å…¥
        x = torch.randn(2, 3, 224, 224).to(device)
        
        # æµ‹è¯•åŸºæœ¬å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            output = model(x)
            print(f"âœ… Basic forward pass: {output.shape}")
        
        # æµ‹è¯•è®­ç»ƒæ¨¡å¼
        model.train()
        main_output, aux_outputs, spec_loss = model(x, return_aux_outputs=True)
        print(f"âœ… Training mode - Main: {main_output.shape}, Aux: {list(aux_outputs.keys())}")
        print(f"âœ… Specialization loss: {spec_loss.item():.4f}")
        
        # æµ‹è¯•æ”¹è¿›çš„è®­ç»ƒç­–ç•¥
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        trainer = ImprovedTrainingStrategy(model, optimizer, device)
        
        targets = torch.randint(0, 2, (2,)).to(device)
        total_loss, loss_dict = trainer.compute_loss(x, targets, epoch=0, total_epochs=100)
        
        print(f"âœ… Improved training strategy:")
        print(f"  - Total loss: {total_loss.item():.4f}")
        print(f"  - Main loss: {loss_dict['main_loss']:.4f}")
        print(f"  - Aux loss: {loss_dict['aux_loss']:.4f}")
        print(f"  - Spec loss: {loss_dict['spec_loss']:.4f}")
        print(f"  - Training stage: {loss_dict['training_stage']}")
        
        # æµ‹è¯•åå‘ä¼ æ’­
        total_loss.backward()
        optimizer.step()
        print("âœ… Backward pass successful!")
        
        # æµ‹è¯•ç‰¹å¾æå–
        model.eval()
        with torch.no_grad():
            features = model.extract_features(x)
            print(f"âœ… Feature extraction: {list(features.keys())}")
        
        print(f"âœ… Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ‰ Improved model testing completed!")


if __name__ == '__main__':
    test_improved_model()